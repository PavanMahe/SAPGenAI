{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b27606b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You asked sap abap on hana\n",
      "Answer: We do have ABAP on HANA cum S/4HANA training available both in video and LIVE mode, please share your email id so we can send details, in meanwhile you can watch below demo video: https://www.youtube.com/watch?v=sf_uw3JMZW4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'We do have ABAP on HANA cum S/4HANA training available both in video and LIVE mode, please share your email id so we can send details, in meanwhile you can watch below demo video: https://www.youtube.com/watch?v=sf_uw3JMZW4'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##3. Vectorize the data of Anubhav Training chatbot\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def process_question(user_input, dataframe):\n",
    "    ##For simplicity, we will just echo back the question\n",
    "    print(f\"You asked {user_input}\")\n",
    "    ##get all the questions in the list\n",
    "    questions = dataframe['Question'].tolist()\n",
    "    #print(f\"Questions in the dataset: {questions}\")\n",
    "    ##Add user's question as first line\n",
    "    questions.append(user_input)\n",
    "    ##Calculate TFIDF for whole data which has my question also\n",
    "    vectorizer = TfidfVectorizer(tokenizer=word_tokenize, stop_words='english')\n",
    "    tfidf_matrix = vectorizer.fit_transform(questions)\n",
    "    ##print the matrix\n",
    "    #print(tfidf_matrix.toarray())\n",
    "    ##Cosine Similarity\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix[-1], tfidf_matrix[:-1])\n",
    "    #print(f\"Cosine Similarity: {cosine_sim}\")\n",
    "\n",
    "    ##Index of the line no of the record which is most similar\n",
    "    most_similar_idx = cosine_sim.argmax()\n",
    "    #print(f\"Most similar question index: {most_similar_idx}\")\n",
    "\n",
    "    ##Remove the question from dataset\n",
    "    questions.pop()\n",
    "\n",
    "    ##Return the answer\n",
    "    answer = dataframe.iloc[most_similar_idx]['Answer']\n",
    "    print(f\"Answer: {answer}\")\n",
    "\n",
    "    ##return the matrix\n",
    "    return answer\n",
    "\n",
    "process_question(\"sap abap on hana\", df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43721926",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\conta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\conta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\conta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Question\n",
      "0  want know training\n",
      "1    want know server\n",
      "2                much\n",
      "3             payment\n",
      "4       share content\n"
     ]
    }
   ],
   "source": [
    "##2 Preprocessing : Clean the data from data set\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "#Download NLTK resources\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "##cleanse data\n",
    "def cleanse_data(data):\n",
    "    word_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "    stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "    cleaned_data = []\n",
    "    for index, row in data.iterrows():\n",
    "        question = row[\"Question\"]\n",
    "        answer = row[\"Answer\"]\n",
    "\n",
    "        # Tokenize and clean question\n",
    "        question_tokens = word_tokenizer.tokenize(question)\n",
    "        question_tokens = [word.lower() for word in question_tokens if word.lower() not in stop_words and word.isalpha()]\n",
    "        question_tokens = [lemmatizer.lemmatize(word) for word in question_tokens]\n",
    "        cleaned_question = \" \".join(question_tokens)\n",
    "\n",
    "        cleaned_data.append({\"Question\": cleaned_question})\n",
    "\n",
    "    return pd.DataFrame(cleaned_data)\n",
    "\n",
    "cleaned_df = cleanse_data(df)\n",
    "print(cleaned_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89b47836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 Question  \\\n",
      "0  I want to know more about the training   \n",
      "1    I want to know more about the server   \n",
      "2                  How much does it cost?   \n",
      "3          What are your payment options?   \n",
      "4            how do you share the content   \n",
      "\n",
      "                                              Answer  \n",
      "0  Please let me know which course are you lookin...  \n",
      "1  Yes, we provide on-demand server access where ...  \n",
      "2  It depends on which module you want to take tr...  \n",
      "3  You can make an online transfer to our bank a/...  \n",
      "4  We will share the private blog access with you...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "##1. load dataset from excel file\n",
    "def load_dataset(path, sheet_name):\n",
    "    ##load the excel\n",
    "    df = pd.read_excel(path, sheet_name=sheet_name)\n",
    "    ##read the intended columns\n",
    "    df = df[['Question','Answer']]\n",
    "    ##Remove rows with missing values\n",
    "    df = df.dropna()\n",
    "    ##Reset the row index after dropping rows\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "df = load_dataset(\"D:\\\\Anubhav_GetAI_Batch\\\\07nlp_ats_chatbot\\\\anubhav_trainings.xlsx\", \"Sheet1\")\n",
    "print(df.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
